{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_LunarLander.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNiMXyxf3kGKlbFejR6PFvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinkopecky98/Checkpoint-1/blob/master/DQN_LunarLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lx1DeVWanPj",
        "outputId": "ef7cae4c-c546-4e6b-cc62-dcf3181da5b5"
      },
      "source": [
        "!pip install gym\r\n",
        "!pip install Box2D\r\n",
        "!pip install torch\r\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting Box2D\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.10\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwrCXG7iPPzV"
      },
      "source": [
        "\r\n",
        "import gym\r\n",
        "from gym import logger as gymlogger\r\n",
        "from gym.wrappers import Monitor\r\n",
        "gymlogger.set_level(40)\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import copy\r\n",
        "import numpy as np\r\n",
        "from random import randrange\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import glob\r\n",
        "import io\r\n",
        "import base64\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_cZleRfTQKhq",
        "outputId": "2d6830b9-a898-46cc-d38f-d47553b98f4f"
      },
      "source": [
        "\r\n",
        "class ExperienceReplay():\r\n",
        "    def __init__(self, size, state_dim):\r\n",
        "        self.index = 0\r\n",
        "        self.size = size\r\n",
        "        self.state_dim = state_dim\r\n",
        "        dim = (size, ) + state_dim\r\n",
        "\r\n",
        "        self.states = torch.zeros(dim)\r\n",
        "        self.actions = torch.zeros((self.size), dtype=torch.int8)\r\n",
        "        self.rewards = torch.zeros(self.size)\r\n",
        "        self.states_ = torch.zeros(dim)\r\n",
        "        self.terminals = torch.zeros(self.size)\r\n",
        "\r\n",
        "    def store(self, state, action, reward, state_, terminal):\r\n",
        "        index = self.index % self.size\r\n",
        "\r\n",
        "        self.states[index] = state\r\n",
        "        self.actions[index] = action\r\n",
        "        self.rewards[index] = reward\r\n",
        "        self.states_[index] = state_\r\n",
        "        self.terminals[index] = int(terminal)\r\n",
        "\r\n",
        "        self.index += 1\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        length = min(self.size, self.index)\r\n",
        "\r\n",
        "        batch = np.random.choice(length, batch_size)\r\n",
        "        states =  self.states[batch]\r\n",
        "        actions = self.actions[batch]\r\n",
        "        rewards = self.rewards[batch]\r\n",
        "        states_ = self.states_[batch]\r\n",
        "        terminal = self.terminals[batch]\r\n",
        "\r\n",
        "        return states, actions, rewards, states_, terminal\r\n",
        "\r\n",
        "class AgentDQN:\r\n",
        "    def __init__(self, gamma, actions_count, model, experience_replay, lr,\r\n",
        "                 update_steps = 1000, batch_size = 64,\r\n",
        "                 epsilon=1.0, epsilon_dec = 1e-4, epsilon_min = 0.01):\r\n",
        "\r\n",
        "        self.gamma = gamma\r\n",
        "        self.actions_count = actions_count\r\n",
        "        self.online_model = model\r\n",
        "        self.target_model = copy.deepcopy(model)\r\n",
        "\r\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "        print('device: ', self.device)\r\n",
        "        self.online_model.to(self.device)\r\n",
        "        self.target_model.to(self.device)\r\n",
        "        for param in self.target_model.parameters():\r\n",
        "            param.requires_grad = False\r\n",
        "        self.mse = nn.MSELoss()\r\n",
        "        self.experience_replay = experience_replay\r\n",
        "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=lr)\r\n",
        "        self.update_steps = update_steps\r\n",
        "        self.current_steps = 0\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epsilon = epsilon\r\n",
        "        self.epsilon_min = epsilon_min\r\n",
        "        self.epsilon_dec = epsilon_dec\r\n",
        "\r\n",
        "    def choose_action(self, state):\r\n",
        "        r = np.random.random()\r\n",
        "\r\n",
        "        if np.random.random() > self.epsilon:\r\n",
        "            action = randrange(self.actions_count)\r\n",
        "            return action\r\n",
        "        else:\r\n",
        "            state = state.unsqueeze(0).to(self.device).float()\r\n",
        "            with torch.no_grad():\r\n",
        "                actions = self.online_model(state)\r\n",
        "            action = torch.argmax(actions).item()\r\n",
        "            return action\r\n",
        "\r\n",
        "    def store(self, state, action, reward, state_, terminal):\r\n",
        "        self.experience_replay.store(state, action, reward, state_, terminal)\r\n",
        "\r\n",
        "    def learn(self):\r\n",
        "        if self.experience_replay.index < 100:\r\n",
        "            return\r\n",
        "\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        states, actions, rewards, states_, terminals = self.experience_replay.sample(self.batch_size)\r\n",
        "\r\n",
        "        q_y = self.online_model(states.to(self.device))\r\n",
        "        q_target = q_y.detach().cpu()\r\n",
        "        q_next = self.target_model(states_.to(self.device)).cpu()\r\n",
        "\r\n",
        "        for i in range(0, len(states)):\r\n",
        "            q_target[i, actions[i]] = rewards[i] + self.gamma * torch.max(q_next[i]) * (1 - terminals[i])\r\n",
        "\r\n",
        "        loss = self.mse(q_y, q_target.to(self.device))\r\n",
        "        loss.backward()\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "        self.current_steps += 1\r\n",
        "        if self.current_steps == self.update_steps:\r\n",
        "            self.target_model.load_state_dict(self.online_model.state_dict())\r\n",
        "            self.current_steps = 0\r\n",
        "\r\n",
        "        if self.epsilon > self.epsilon_min:\r\n",
        "            self.epsilon = max(self.epsilon - self.epsilon_dec, self.epsilon_min)\r\n",
        "\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(8,8)\r\n",
        "        self.fc2 = nn.Linear(8,8)\r\n",
        "        self.fc3 = nn.Linear(8,4)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "env = gym.make('LunarLander-v2')\r\n",
        "actions = 4\r\n",
        "state_dim = (8, )\r\n",
        "\r\n",
        "# env = gym.make('LunarLander-v2')\r\n",
        "# actions = 3\r\n",
        "# state_dim = (2, )\r\n",
        "\r\n",
        "\r\n",
        "games_count = 500\r\n",
        "scores = []\r\n",
        "\r\n",
        "experience_replay = ExperienceReplay(10000, state_dim)\r\n",
        "agent = AgentDQN(0.99, actions, Net(), experience_replay, 0.001)\r\n",
        "\r\n",
        "for i in range(0, games_count):\r\n",
        "\r\n",
        "    score = 0\r\n",
        "    terminal = False\r\n",
        "    state = env.reset()\r\n",
        "    state = torch.from_numpy(state).double()\r\n",
        "\r\n",
        "    while not terminal:\r\n",
        "        action = agent.choose_action(state)\r\n",
        "        state_, reward, terminal, _ = env.step(action)\r\n",
        "        state_ = torch.from_numpy(state_).double()\r\n",
        "        agent.experience_replay.store(state, action, reward, state_, terminal)\r\n",
        "        \r\n",
        "        #doplniť metódu učenia\r\n",
        "        agent.learn()\r\n",
        "        state = state_\r\n",
        "        score += reward\r\n",
        "\r\n",
        "    scores.append(score)\r\n",
        "\r\n",
        "    if i % 5 == 0:\r\n",
        "      print('episode: ', i, '\\t\\tscore: ', + score, '\\t\\taverage score:' , np.average(scores[-100:]), '\\t\\tepsilon: ', agent.epsilon)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device:  cpu\n",
            "episode:  0 \t\tscore:  -156.8139133509145 \t\taverage score: -156.8139133509145 \t\tepsilon:  1.0\n",
            "episode:  5 \t\tscore:  -158.04975927851325 \t\taverage score: -137.1479602037306 \t\tepsilon:  0.9665000000000037\n",
            "episode:  10 \t\tscore:  -155.62877738714585 \t\taverage score: -136.22209597337903 \t\tepsilon:  0.927400000000008\n",
            "episode:  15 \t\tscore:  -100.62229401649964 \t\taverage score: -133.62795004489533 \t\tepsilon:  0.8889000000000122\n",
            "episode:  20 \t\tscore:  -139.07277212711907 \t\taverage score: -140.52502157930397 \t\tepsilon:  0.8515000000000164\n",
            "episode:  25 \t\tscore:  -115.6785673148405 \t\taverage score: -139.6296186052805 \t\tepsilon:  0.8120000000000207\n",
            "episode:  30 \t\tscore:  -114.1737207201602 \t\taverage score: -142.14894925888353 \t\tepsilon:  0.773400000000025\n",
            "episode:  35 \t\tscore:  -127.71987916706266 \t\taverage score: -141.2273965676333 \t\tepsilon:  0.7343000000000293\n",
            "episode:  40 \t\tscore:  -110.14686592881245 \t\taverage score: -140.9771847881115 \t\tepsilon:  0.6953000000000336\n",
            "episode:  45 \t\tscore:  -73.32915972814475 \t\taverage score: -138.56059782343797 \t\tepsilon:  0.654700000000038\n",
            "episode:  50 \t\tscore:  -86.46891632698978 \t\taverage score: -135.5192518816101 \t\tepsilon:  0.6153000000000424\n",
            "episode:  55 \t\tscore:  -282.10267104767286 \t\taverage score: -137.9669548623913 \t\tepsilon:  0.5674000000000476\n",
            "episode:  60 \t\tscore:  -109.3987397809064 \t\taverage score: -140.22199283992728 \t\tepsilon:  0.527900000000052\n",
            "episode:  65 \t\tscore:  39.530841935416476 \t\taverage score: -138.1637610763272 \t\tepsilon:  0.4952000000000556\n",
            "episode:  70 \t\tscore:  -316.6543025550918 \t\taverage score: -140.5999368458937 \t\tepsilon:  0.45290000000006025\n",
            "episode:  75 \t\tscore:  10.361110048306898 \t\taverage score: -138.71130833749064 \t\tepsilon:  0.41190000000006477\n",
            "episode:  80 \t\tscore:  -238.27636758826688 \t\taverage score: -138.0796151178282 \t\tepsilon:  0.3771000000000686\n",
            "episode:  85 \t\tscore:  -125.67348919365772 \t\taverage score: -137.2884150263534 \t\tepsilon:  0.3352000000000732\n",
            "episode:  90 \t\tscore:  -103.78900731758878 \t\taverage score: -137.7396555457697 \t\tepsilon:  0.2944000000000777\n",
            "episode:  95 \t\tscore:  -360.85761279760516 \t\taverage score: -139.21345670013827 \t\tepsilon:  0.2472000000000829\n",
            "episode:  100 \t\tscore:  -41.94498389027109 \t\taverage score: -138.08287524223093 \t\tepsilon:  0.2046000000000876\n",
            "episode:  105 \t\tscore:  -208.21908762835238 \t\taverage score: -141.2681670561016 \t\tepsilon:  0.15530000000009303\n",
            "episode:  110 \t\tscore:  -306.7591041767413 \t\taverage score: -144.70128997950488 \t\tepsilon:  0.11100000000009597\n",
            "episode:  115 \t\tscore:  -121.34833895801708 \t\taverage score: -147.99577175838974 \t\tepsilon:  0.06740000000009472\n",
            "episode:  120 \t\tscore:  -78.09823289564173 \t\taverage score: -145.6755791558672 \t\tepsilon:  0.021600000000093742\n",
            "episode:  125 \t\tscore:  -87.16155643144558 \t\taverage score: -150.7221124716612 \t\tepsilon:  0.01\n",
            "episode:  130 \t\tscore:  -390.9105021859601 \t\taverage score: -151.0686899631454 \t\tepsilon:  0.01\n",
            "episode:  135 \t\tscore:  -143.3706169812263 \t\taverage score: -151.87629386902321 \t\tepsilon:  0.01\n",
            "episode:  140 \t\tscore:  -91.33156372015088 \t\taverage score: -151.6103290539938 \t\tepsilon:  0.01\n",
            "episode:  145 \t\tscore:  -75.59340494868354 \t\taverage score: -158.69997441794376 \t\tepsilon:  0.01\n",
            "episode:  150 \t\tscore:  -166.05084713414016 \t\taverage score: -162.84520639262155 \t\tepsilon:  0.01\n",
            "episode:  155 \t\tscore:  -120.98308723741665 \t\taverage score: -164.2643917852192 \t\tepsilon:  0.01\n",
            "episode:  160 \t\tscore:  -142.32489972430037 \t\taverage score: -164.5361058468476 \t\tepsilon:  0.01\n",
            "episode:  165 \t\tscore:  -292.5671055338647 \t\taverage score: -165.7824848740422 \t\tepsilon:  0.01\n",
            "episode:  170 \t\tscore:  -101.05295368396011 \t\taverage score: -165.2093713559665 \t\tepsilon:  0.01\n",
            "episode:  175 \t\tscore:  -123.48367928705525 \t\taverage score: -166.19770042928806 \t\tepsilon:  0.01\n",
            "episode:  180 \t\tscore:  -261.55786911440606 \t\taverage score: -171.97700052389808 \t\tepsilon:  0.01\n",
            "episode:  185 \t\tscore:  -58.55279255421301 \t\taverage score: -170.14352531197108 \t\tepsilon:  0.01\n",
            "episode:  190 \t\tscore:  -429.20003716923753 \t\taverage score: -170.96797750043714 \t\tepsilon:  0.01\n",
            "episode:  195 \t\tscore:  -215.6038977761089 \t\taverage score: -171.88344467104318 \t\tepsilon:  0.01\n",
            "episode:  200 \t\tscore:  -359.660643817277 \t\taverage score: -174.75408790369545 \t\tepsilon:  0.01\n",
            "episode:  205 \t\tscore:  -107.86268942842455 \t\taverage score: -172.26849994065077 \t\tepsilon:  0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a8226f11ab79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m#doplniť metódu učenia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a8226f11ab79>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__rsub__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}